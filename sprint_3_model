# =============================
# INST414 - Sprint 3 (Google Colab Version)
# Hate Speech Classification with HateBERT + EDA + GPT-4.1-nano Categorization
# =============================

# ‚úÖ Mount Google Drive (optional)
from google.colab import drive
import os

drive.mount('/content/drive')  # comment out if not needed

# ‚úÖ Install Dependencies
!pip install transformers torch pandas scikit-learn datasets tabulate matplotlib seaborn tqdm openai --quiet

# ‚úÖ Imports
import getpass
import re, json, time, torch
import pandas as pd
import numpy as np
from pathlib import Path
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from tabulate import tabulate
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, cohen_kappa_score
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import openai
from google.colab import files

# ‚úÖ Ensure Plots Folder
os.makedirs("plots", exist_ok=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)

# Persistent Drive output directory
drive_output_dir = "/content/drive/MyDrive/INST414 - Social Media NLP Project"
os.makedirs(drive_output_dir, exist_ok=True)

# ‚úÖ Load HateBERT
print("Loading HateBERT model...")
tokenizer = AutoTokenizer.from_pretrained("Hate-speech-CNERG/dehatebert-mono-english")
model = AutoModelForSequenceClassification.from_pretrained("Hate-speech-CNERG/dehatebert-mono-english").to(device)
model.eval()

# ‚úÖ Upload Dataset
uploaded = files.upload()
filepath = next(iter(uploaded))

# =============================
# Core Functions
# =============================

def load_dataframe(filepath):
    df = pd.read_csv(filepath)
    print(f"Loaded dataset with shape: {df.shape}")
    print(df.head())
    return df

def clean_tweet(tweet):
    tweet = re.sub(r"@\w+", "", tweet)
    tweet = re.sub(r"http\S+|www\S+|https\S+", "", tweet)
    tweet = re.sub(r"\s+", " ", tweet).strip()
    return tweet

def clean_dataframe(df, text_column="tweet"):
    df["cleaned_tweet"] = df[text_column].apply(clean_tweet)
    return df

def compare_dataframes(df):
    print("\nSample of original vs. cleaned tweets:")
    print(df[["tweet", "cleaned_tweet"]].head())

def plot_label_distribution(df):
    label_map = {0: "hate_speech", 1: "offensive", 2: "neither"}
    df["label_text"] = df["class"].map(label_map)
    sns.countplot(data=df, x="label_text", order=["hate_speech", "offensive", "neither"])
    plt.title("Annotator Label Distribution")
    plt.tight_layout()
    plt.savefig("plots/annotator_label_distribution.png")  # save here
    plt.close()

def plot_tweet_length_distribution(df):
    df["tweet_length"] = df["tweet"].apply(len)
    sns.histplot(df["tweet_length"], bins=30, kde=True)
    plt.title("Tweet Length Distribution")
    plt.tight_layout()
    plt.savefig("plots/tweet_length_distribution.png")
    plt.close()

def show_top_tfidf_words(df, label_value, label_column="class", text_column="cleaned_tweet", top_n=15):
    subset = df[df[label_column] == label_value]
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(subset[text_column])
    means = tfidf_matrix.mean(axis=0).A1
    words = vectorizer.get_feature_names_out()
    top_indices = means.argsort()[::-1][:top_n]
    for i in top_indices:
        print(f"{words[i]}: {means[i]:.3f}")

def classify_tweet_hatebert(df, text_column="cleaned_tweet", batch_size=32):
    label_map = {0: "hate_speech", 1: "offensive", 2: "neither"}
    predictions = []
    for i in tqdm(range(0, len(df), batch_size), desc="Classifying with HateBERT"):
        batch_texts = df[text_column].iloc[i:i+batch_size].tolist()
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().tolist()
            predictions.extend(preds)
    df["hatebert_label"] = predictions
    df["hatebert_label_text"] = df["hatebert_label"].map(label_map)
    return df

def plot_prediction_distribution(df):
    sns.countplot(data=df, x="hatebert_label_text", order=["hate_speech", "offensive", "neither"])
    plt.title("HateBERT Predicted Label Distribution")
    plt.tight_layout()
    plt.savefig("plots/hatebert_label_distribution.png")
    plt.close()

def plot_annotator_vs_hatebert_comparison(df):
    label_map = {0: "hate_speech", 1: "offensive", 2: "neither"}
    df["annotator_label_text"] = df["class"].map(label_map)
    melted = pd.melt(df, value_vars=["annotator_label_text", "hatebert_label_text"],
                     var_name="Source", value_name="Label")
    sns.countplot(data=melted, x="Label", hue="Source", order=["hate_speech", "offensive", "neither"])
    plt.title("Annotator vs. HateBERT Prediction Distribution")
    plt.tight_layout()
    plt.savefig("plots/annotator_vs_hatebert.png")
    plt.show()

def plot_confusion(df):
    labels = [0, 1, 2]
    cm = confusion_matrix(df["class"], df["hatebert_label"], labels=labels)
    disp = ConfusionMatrixDisplay(cm, display_labels=["hate_speech", "offensive", "neither"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix (Counts)")
    plt.savefig("plots/confusion_matrix_counts.png")
    plt.close()

    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    disp_norm = ConfusionMatrixDisplay(cm_norm, display_labels=["hate_speech", "offensive", "neither"])
    disp_norm.plot(cmap=plt.cm.Oranges, values_format=".2f")
    plt.title("Confusion Matrix (Percentages)")
    plt.savefig("plots/confusion_matrix_percentages.png")
    plt.close()

# üîê Replace this with your actual OpenAI key
def classify_with_openai_subcategories(df, max_tweets=600):
    import getpass
    openai.api_key = getpass.getpass("Enter your OpenAI API key: ")

    prompt_template = """You are a hate speech expert. Categorize the following tweet into one or more of these categories:
- Racism
- Sexism / Misogyny
- Homophobia / Transphobia
- Religious Hate
- Xenophobia
- Ageism
- Ableism

Tweet: "{tweet}"

Return a JSON array of matching categories. If none, return an empty array.
Example: ["Racism"]
"""

    # Ensure column exists
    if "hate_subcategories" not in df.columns:
        df["hate_subcategories"] = None

    hate_tweets = df[df["hatebert_label"] == 0].head(max_tweets)

    for i, row in tqdm(hate_tweets.iterrows(), total=len(hate_tweets), desc="Classifying with OpenAI"):
        if pd.notna(row.get("hate_subcategories")) and row["hate_subcategories"] not in ["[]", ""]:
            continue  # Skip if already classified

        tweet = row["cleaned_tweet"]
        try:
            response = openai.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a hate speech classification assistant."},
                    {"role": "user", "content": prompt_template.format(tweet=tweet)}
                ],
                temperature=0.2,
            )
            categories = json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"Error on index {i}: {e}")
            categories = []
        df.at[i, "hate_subcategories"] = categories

    return df

plot_label_distribution(df)
plot_tweet_length_distribution(df)

print("\nTF-IDF for hate speech:")
show_top_tfidf_words(df, 0)

print("\nTF-IDF for offensive speech:")
show_top_tfidf_words(df, 1)

print("üîÑ Checking for HateBERT cache...")
hatebert_cache = Path("hatebert_output.csv")

if hatebert_cache.exists():
    print("‚úÖ Loaded cached HateBERT+OpenAI results.")
    df = pd.read_csv(hatebert_cache)
else:
    print("‚öôÔ∏è HateBERT not found ‚Äî running classification...")
    df = load_dataframe(filepath)
    df = clean_dataframe(df)
    compare_dataframes(df)
    df = classify_tweet_hatebert(df)
    df.to_csv(hatebert_cache, index=False)
    print("‚úÖ HateBERT results saved to cache.")

# Run OpenAI on up to N hate speech tweets
df = classify_with_openai_subcategories(df, max_tweets=600)  # or max_hate later
df.to_csv("hatebert_openai_categorized.csv", index=False)

print(df["hatebert_label_text"].value_counts())
plot_prediction_distribution(df)
plot_annotator_vs_hatebert_comparison(df)

print("\nüîç Confusion Matrix:")
plot_confusion(df)

!cp plots/*.png /content/drive/MyDrive/INST414 - Social Media NLP Project/

print("\nüìä Classification Report:")
print(classification_report(df["class"], df["hatebert_label"], target_names=["hate_speech", "offensive", "neither"]))

print("\nü§ù Cohen‚Äôs Kappa:")
print(cohen_kappa_score(df["class"], df["hatebert_label"]))

!cp hatebert_openai_categorized.csv /content/drive/MyDrive/INST414 - Social Media NLP Project/
print("Saved to hatebert_openai_categorized.csv")


print("\nTop subcategories (frequency):")
print(df["hate_subcategories"].explode().value_counts())


# final downloads
# ‚úÖ Save both CSVs to Google Drive
!cp hatebert_openai_categorized.csv "$drive_output_dir/"

# ‚úÖ Save all plots
!cp plots/*.png "$drive_output_dir/"

# ‚úÖ Zip everything in the output directory
!zip -r /content/drive/MyDrive/INST414_Project_Archive.zip "$drive_output_dir"
files.download("hatebert_openai_categorized.csv")
!zip -r /content/drive/MyDrive/INST414_Project_Archive.zip "$drive_output_dir"
